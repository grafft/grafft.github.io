---
layout: post
date: 2026-02-19 12:00:00+0000
inline: true
related_posts: false
---

Four papers from our lab have been accepted to <a href='https://iclr.cc/'>ICLR 2026</a>:

<strong><a href='https://arxiv.org/abs/2306.09459v4'>Recurrent Action Transformer with Memory</a></strong> — A new architecture that integrates recurrent memory mechanisms into transformer-based offline reinforcement learning, addressing the quadratic complexity limitation of attention and improving performance on memory-dependent tasks.

<strong><a href='https://arxiv.org/abs/2510.07151'>ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL</a></strong> — A transformer-based approach with structured external memory that extends effective horizons up to 100,000 times beyond the attention window, nearly doubling performance on real robotic manipulation tasks.

<strong><a href='https://arxiv.org/abs/2412.06531'>Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation</a></strong> — A systematic study proposing practical definitions and standardized evaluation methods for memory mechanisms in reinforcement learning agents.

<strong><a href='https://arxiv.org/abs/2502.10550v2'>Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning</a></strong> — Introducing MIKASA, a comprehensive benchmark framework for evaluating memory capabilities in RL, including MIKASA-Robo with 32 robotic manipulation tasks.
