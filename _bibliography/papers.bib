---
---
@string{aps = {American Physical Society,}}

@article{Kovalev2022,
  bibtex_show={true},
  abbr={Doklady},
  dimensions={true},
  selected={false},
	title = {Application of Pretrained Large Language Models in Embodied Artificial Intelligence},
	volume = {106},
	issn = {1064-5624},
	url = {https://link.springer.com/10.1134/S1064562422060138},
	doi = {10.1134/S1064562422060138},
	abstract = {A feature of tasks in embodied artificial intelligence is that a query to an intelligent agent is formulated in natural language. As a result, natural language processing methods have to be used to transform the query into a format convenient for generating an appropriate action plan. There are two basic approaches to the solution of this problem. One is based on specialized models trained with particular instances of instructions translated into agent-executable format. The other approach relies on the ability of large language models trained with a large amount of unlabeled data to store common sense knowledge. As a result, such models can be used to generate an agent’s action plan in natural language without preliminary learning. This paper provides a detailed review of models based on the second approach as applied to embodied artificial intelli- gence tasks.},
	pages = {S85--S90},
	issue = {S1},
	journaltitle = {Doklady Mathematics},
	author = {Kovalev, A. K. and Panov, Aleksandr I.},
	date = {2022},
	keywords = {slpa, group1}
}

@article{Dzhivelikian2022a,
  bibtex_show={true},
  abbr={BrainInf},
  dimensions={true},
  selected={true},
	title = {Hierarchical intrinsically motivated agent planning behavior with dreaming in grid environments},
	volume = {9},
	issn = {2198-4018},
	url = {https://braininformatics.springeropen.com/articles/10.1186/s40708-022-00156-6},
	doi = {10.1186/s40708-022-00156-6},
	abstract = {Biologically plausible models of learning may provide a crucial insight for building autonomous intelligent agents capable of performing a wide range of tasks. In this work, we propose a hierarchical model of an agent operating in an unfamiliar environment driven by a reinforcement signal. We use temporal memory to learn sparse distributed representation of state–actions and the basal ganglia model to learn effective action policy on different levels of abstraction. The learned model of the environment is utilized to generate an intrinsic motivation signal, which drives the agent in the absence of the extrinsic signal, and through acting in imagination, which we call dreaming. We demonstrate that the proposed architecture enables an agent to effectively reach goals in grid environments.},
	pages = {8},
	number = {1},
	journaltitle = {Brain Informatics},
	author = {Dzhivelikian, Evgenii and Latyshev, Artem and Kuderov, Petr and Panov, Aleksandr I},
	date = {2022},
	keywords = {slap, group1}
}

@incollection{Ugadiarov2021,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Long-Term Exploration in Persistent {MDPs}},
	volume = {13067},
	isbn = {978-3-030-89817-5},
	url = {http://arxiv.org/abs/2109.10173},
	abstract = {Exploration is an essential part of reinforcement learning, which restricts the quality of learned policy. Hard-exploration environments are defined by huge state space and sparse rewards. In such conditions, an exhaustive exploration of the environment is often impossible, and the successful training of an agent requires a lot of interaction steps. In this paper, we propose an exploration method called Rollback-Explore ({RbExplore}), which utilizes the concept of the persistent Markov decision process, in which agents during training can roll back to visited states. We test our algorithm in the hard-exploration Prince of Persia game, without rewards and domain knowledge. At all used levels of the game, our agent outperforms or shows comparable results with state-of-the-art curiosity methods with knowledge-based intrinsic motivation: {ICM} and {RND}. An implementation of {RbExplore} can be found at https://github.com/cds-mipt/{RbExplore}.},
	pages = {108--120},
	booktitle = {Advances in Soft Computing. {MICAI} 2021. Part I. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Ugadiarov, Leonid and Skrynnik, Alexey and Panov, Aleksandr I.},
	editor = {Batyrshin, Ildar and Gelbukh, Alexander and Sidorov, Grigori},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2109.10173},
	doi = {10.1007/978-3-030-89817-5_8},
	keywords = {slap, group1}
}

@incollection{Rak2021,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Flexible Data Augmentation in Off-Policy Reinforcement Learning},
	volume = {12854},
	url = {https://www.scopus.com/record/display.uri?eid=2-s2.0-85117454790&origin=resultslist},
	abstract = {This paper explores an application of image augmentation in reinforcement learning tasks - a popular regularization technique in the computer vision area. The analysis is based on the model-free off-policy algorithms. As a regularization, we consider the augmentation of the frames that are sampled from the replay buffer of the model. Evaluated augmentation techniques are random changes in image contrast, random shifting, random cutting, and others. Research is done using the environments of the Atari games: Breakout, Space Invaders, Berzerk, Wizard of Wor, Demon Attack. Using augmentations allowed us to obtain results confirming the significant acceleration of the model’s algorithm convergence. We also proposed an adaptive mechanism for selecting the type of augmentation depending on the type of task being performed by the agent.},
	pages = {224--235},
	booktitle = {Artificial Intelligence and Soft Computing. {ICAISC} 2021. Lecture Notes in Computer Science},
	publisher = {Springer, Cham},
	author = {Rak, Alexandra and Skrynnik, Alexey and Panov, Aleksandr I},
	editor = {Rutkowski, L.},
	date = {2021},
	doi = {10.1007/978-3-030-87986-0_20},
	keywords = {slpa, group1}
}

@article{Skrynnik2021,
  bibtex_show={true},
  abbr={KBS},
  dimensions={true},
  selected={true},
	title = {Forgetful experience replay in hierarchical reinforcement learning from expert demonstrations},
	volume = {218},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121001076},
	doi = {10.1016/j.knosys.2021.106844},
	abstract = {Deep reinforcement learning ({RL}) shows impressive results in complex gaming and robotic environments. These results are commonly achieved at the expense of huge computational costs and require an incredible number of episodes of interactions between the agent and the environment. Hierarchical methods and expert demonstrations are among the most promising approaches to improve the sample efficiency of reinforcement learning methods. In this paper, we propose a combination of methods that allow the agent to use low-quality demonstrations in complex vision-based environments with multiple related goals. Our Forgetful Experience Replay ({ForgER}) algorithm effectively handles expert data errors and reduces quality losses when adapting the action space and states representation to the agent’s capabilities. The proposed goal-oriented replay buffer structure allows the agent to automatically highlight sub-goals for solving complex hierarchical tasks in demonstrations. Our method has a high degree of versatility and can be integrated into various off-policy methods. The {ForgER} surpasses the existing state-of-the-art {RL} methods using expert demonstrations in complex environments. The solution based on our algorithm beats other solutions for the famous {MineRL} competition and allows the agent to demonstrate the behavior at the expert level.},
	pages = {106844},
	journaltitle = {Knowledge-Based Systems},
	author = {Skrynnik, Alexey and Staroverov, Aleksey and Aitygulov, Ermek and Aksenov, Kirill and Davydov, Vasilii and Panov, Aleksandr I.},
	date = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {slap, group1}
}

@article{Skrynnik2021a,
  bibtex_show={true},
  abbr={CSR},
  dimensions={true},
  selected={true},
	title = {Hierarchical Deep Q-Network from imperfect demonstrations in Minecraft},
	volume = {65},
	issn = {13890417},
	url = {https://www.sciencedirect.com/science/article/pii/S1389041720300723?via%3Dihub},
	doi = {10.1016/j.cogsys.2020.08.012},
	abstract = {We present hierarchical Deep Q-Network ({HDQfD}) that took first place in {MineRL} competition. {HDQfD} works on imperfect demonstrations utilize hierarchical structure of expert trajectories extracting effective sequence of meta-actions and subgoals. We introduce structured task dependent replay buffer and adaptive prioritizing technique that allow the {HDQfD} agent to gradually erase poor-quality expert data from the buffer. In this paper we present the details of the {HDQfD} algorithm and give the experimental results in Minecraft domain.},
	pages = {74--78},
	journaltitle = {Cognitive Systems Research},
	author = {Skrynnik, Alexey and Staroverov, Aleksey and Aitygulov, Ermek and Aksenov, Kirill and Davydov, Vasilii and Panov, Aleksandr I.},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {1912.08664v2},
	keywords = {slap, group1}
}

@inproceedings{Kiselev2018c,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Task and Spatial Planning by the Cognitive Agent with Human-like Knowledge Representation},
	volume = {11097},
	isbn = {978-3-319-99582-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-99582-3_1},
	doi = {10.1007/978-3-319-99582-3_1},
	abstract = {The paper considers the task of simultaneous learning and planning actions for moving a cognitive agent in two-dimensional space. Planning is carried out by an agent who uses an anthropic way of knowledge representation that allows him to build transparent and understood planes, which is especially important in case of human-machine interaction. Learning actions to manipulate objects is carried out through reinforcement learning and demonstrates the possibilities of replenishing the agent’s procedural knowledge. The presented approach was demonstrated in an experiment in the Gazebo simulation environment.},
	pages = {1--12},
	booktitle = {Interactive Collaborative Robotics. {ICR} 2018. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Aitygulov, Ermek and Kiselev, Gleb and Panov, Aleksandr I.},
	editor = {Ronzhin, A. and Rigoll, G. and Meshcheryakov, R.},
	date = {2018},
	keywords = {slap, group1}
}

@article{Emelyanov2016,
  bibtex_show={true},
  abbr={CSR},
  dimensions={true},
  selected={true},
	title = {Multilayer cognitive architecture for {UAV} control},
	volume = {39},
	issn = {1389-0417},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041716000048},
	doi = {10.1016/j.cogsys.2015.12.008},
	abstract = {Extensive use of unmanned aerial vehicles ({UAVs}) in recent years has induced the rapid growth of research areas related to {UAV} production. Among these, the design of control systems capable of automating a wide range of {UAV} activities is one of the most actively explored and evolving. Currently, researchers and developers are interested in designing control systems that can be referred to as intelligent, e.g. the systems which are suited to solve such tasks as planning, goal prioritization, coalition formation etc. and thus guarantee high levels of {UAV} autonomy. One of the principal problems in intelligent control system design is tying together various methods and models traditionally used in robotics and aimed at solving such tasks as dynamics modelling, control signal genera- tion, location and mapping, path planning etc. with the methods of behaviour modelling and planning which are thoroughly studied in cognitive science. Our work is aimed at solving this problem. We propose layered architecture — {STRL} (strategic, tactical, reactive, layered) — of the control system that au- tomates the behaviour generation using a cognitive approach while taking into account complex dynamics and kinematics of the control object ({UAV}).We use a special type of knowledge representation — sign world model — that is based on the psychological activity theory to describe individual behaviour planning and coalition formation processes. We also propose path planning methodology which serves as the mediator between the high-level cognitive activities and the reactive control signals generation. To generate these signals we use a state-dependent Riccati equation and specific method for solving it. We believe that utilization of the proposed architecture will broaden the spectrum of tasks which can be solved by the {UAV}’s coalition automatically, as well as raise the autonomy level of each individual member of that coalition.},
	pages = {58--72},
	journaltitle = {Cognitive Systems Research},
	author = {Emel’yanov, Stanislav and Makarov, Dmitry and Panov, Aleksandr I. and Yakovlev, Konstantin},
	date = {2016},
	keywords = {strl, group1}
}

@incollection{Kovalev2021b,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Applying Vector Symbolic Architecture and Semiotic Approach to Visual Dialog},
	volume = {12886},
	isbn = {978-3-030-86270-1},
	url = {https://www.scopus.com/record/display.uri?eid=2-s2.0-85115865440&origin=resultslist},
	abstract = {The multi-modal tasks have started to play a significant role in the research on Artificial Intelligence. A particular example of that domain is visual-linguistic tasks, such as Visual Question Answering and its extension, Visual Dialog. In this paper, we concentrate on the Visual Dialog task and dataset. The task involves two agents. The first agent does not see an image and asks questions about the image content. The second agent sees this image and answers questions. The symbol grounding problem, or how symbols obtain their meanings, plays a crucial role in such tasks. We approach that problem from the semiotic point of view and propose the Vector Semiotic Architecture for Visual Dialog. The Vector Semiotic Architecture is a combination of the Sign-Based World Model and Vector Symbolic Architecture. The Sign-Based World Model represents agent knowledge on the high level of abstraction and allows uniform representation of different aspects of knowledge, forming a hierarchical representation of that knowledge in the form of a special kind of semantic network. The Vector Symbolic Architecture represents the computational level and allows to operate with symbols as with numerical vectors using simple element-wise operations. That combination enables grounding object representation from any level of abstraction to the sensory agent input.},
	pages = {243--255},
	booktitle = {Hybrid Artificial Intelligent Systems. {HAIS} 2021. Lecture Notes in Computer Science},
	author = {Kovalev, Alexey K and Shaban, Makhmud and Chuganskaya, Anfisa A. and Panov, Aleksandr I.},
	editor = {González, Hugo Sanjurjo and López, Iker Pastor and Bringa, Pablo García and Quintián, {SHéctor} and Corchado, Emilio},
	date = {2021},
	doi = {10.1007/978-3-030-86271-8_21},
	keywords = {swm, group1}
}

@inproceedings{Kovalev2019,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Mental Actions and Modelling of Reasoning in Semiotic Approach to {AGI}},
	volume = {11654},
	isbn = {978-3-030-27005-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-27005-6_12},
	doi = {10.1007/978-3-030-27005-6_12},
	abstract = {The article expounds the functional of a cognitive architecture Sign-Based World Model ({SBWM}) through the algorithm for the implementation of a particular case of reasoning. The {SBWM} architecture is a multigraph, called a semiotic network with special rules of activation spreading. In a semiotic network, there are four subgraphs that have specific properties and are composed of constituents of the main {SBWM} element – the sign. Such subgraphs are called causal networks on images, significances, personal meanings, and names. The semiotic network can be viewed as the memory of an intelligent agent. It is proposed to divide the agent’s memory in the {SBWM} architecture into a long-term memory consisting of signs-prototype, and a working memory consisting of signs-instance. The concept of elementary mental actions is introduced as an integral part of the reasoning process. Examples of such actions are provided. The performance of the proposed reasoning algorithm is considered by a model example.},
	pages = {121--131},
	booktitle = {Artificial General Intelligence. {AGI} 2019. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Kovalev, Alexey K and Panov, Aleksandr I},
	editor = {Hammer, Patrick and Agrawal, Pulin and Goertzel, Ben and Iklé, Matthew},
	date = {2019},
	keywords = {swm, group1}
}

@inproceedings{Kiselev2019,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Hierarchical Psychologically Inspired Planning for Human-Robot Interaction Tasks},
	volume = {11659},
	isbn = {978-3-030-26118-4},
	url = {https://doi.org/10.1007/978-3-030-26118-4_15},
	doi = {10.1007/978-3-030-26118-4_15},
	abstract = {This paper presents a new algorithm for hierarchical case-based behavior planning in a coalition of agents – {HierMAP}. The considered algorithm, in contrast to the well-known planners {HEART}, {PANDA}, and others, is intended primarily for use in multi-agent tasks. For this, the possibility of dynamically distributing agent roles with different functionalities was realized. The use of a psychologically plausible approach to the representation of the knowledge by agents using a semiotic network allows applying {HierMAP} in groups in which people participate as one of the actors. Thus, the algorithm allows us to represent solutions of collaborative problems, forming human- interpretable results at each planning step. Another advantage of the proposed method is the ability to save and reuse experience of planning – expansion in the field of case-based planning. Such extension makes it possible to consider information about the success/ failure of interaction with other members of the coalition. Presenting precedents as a special part of the agent’s memory (semantic network on meanings) allows to significantly reduce the planning time for a similar class of tasks. The paper deals with smart relocation tasks in the environment. A comparison is made with the main hierarchical planners widely used at present.},
	pages = {150--160},
	booktitle = {Interactive Collaborative Robotics. {ICR} 2019. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Kiselev, Gleb and Panov, Aleksandr},
	editor = {Ronzhin, Andrey and Rigoll, Gerhard and Meshcheryakov, Roman},
	date = {2019},
	keywords = {swm, group1}
}

@article{Panov2019,
  bibtex_show={true},
  abbr={STIP},
  dimensions={true},
  selected={false},
	title = {Goal Setting and Behavior Planning for Cognitive Agents},
	volume = {46},
	url = {https://link.springer.com/article/10.3103/S0147688219060066},
	doi = {10.3103/S0147688219060066},
	pages = {404--415},
	number = {6},
	journaltitle = {Scientific and Technical Information Processing},
	author = {Panov, Aleksandr I.},
	date = {2019},
	keywords = {swm, group1}
}

@article{Osipov2014b,
  bibtex_show={true},
  abbr={JCSC},
  dimensions={true},
  selected={false},
	title = {Behavior control as a function of consciousness. I. World model and goal setting},
	volume = {53},
	issn = {1064-2307},
	url = {http://link.springer.com/10.1134/S1064230714040121},
	doi = {10.1134/S1064230714040121},
	abstract = {Functions that are referred in psychology as functions of consciousness are considered. These functions include reflection, consciousness of activity motivation, goal setting, synthesis of goal oriented behavior, and some others. The description is based on the concept of sign, which is widely used in psychology and, in particular, in the cultural-historical theory by Vygotsky, in which sign is interpreted informally. In this paper, we elaborate upon the concept of sign, consider mechanisms of sign formation, and some self-organization on the set of signs. Due to the work of self-organization mechanisms, a new method for the representation of the world model of an actor appears. The concept of semiotic network is introduced that is used for the examination of the actor’s world models. Models of some functions indicated above are constructed. The second part of the paper is devoted to functions of self-consciousness and to the application of the constructed models for designing plans and constructing new architectures of intelligent agents that are able, in particular, to distribute roles in coalitions.},
	pages = {517--529},
	number = {4},
	journaltitle = {Journal of Computer and Systems Sciences International},
	author = {Osipov, G. S. and Panov, A. I. and Chudova, N. V.},
	urldate = {2014-09-29},
	date = {2014},
	keywords = {swm, group1}
}

@article{Skrynnik2023,
  bibtex_show={true},
  abbr={TNNLS},
  dimensions={true},
  selected={true},
	title = {When to Switch: Planning and Learning For Partially Observable Multi-Agent Pathfinding},
	doi = {10.1109/TNNLS.2023.3303502},
	pages = {(In Press)},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{TNNLS}},
	author = {Skrynnik, Alexey and Andreychuk, Anton and Yakovlev, Konstantin and Panov, Aleksandr},
	date = {2023},
	langid = {english},
	keywords = {appl, group1}
}


@article{Yakovkev2022,
  bibtex_show={true},
  abbr={Doklady},
  dimensions={true},
  selected={false},
	title = {Planning and Learning in Multi-Agent Path Finding},
	volume = {106},
	issn = {1064-5624},
	url = {https://link.springer.com/10.1134/S1064562422060229},
	doi = {10.1134/S1064562422060229},
	abstract = {Multi-agent path finding arises, on the one hand, in numerous applied areas. A classical example is automated warehouses with a large number of mobile goods-sorting robots operating simultaneously. On the other hand, for this problem, there are no universal solution methods that simultaneously satisfy numerous (often contradictory) requirements. Examples of such criteria are a guarantee of finding optimal solu- tions, high-speed operation, the possibility of operation in partially observable environments, etc. This paper provides a survey of modern methods for multi-agent path finding. Special attention is given to various settings of the problem. The differences and between trainable and nontrainable solution methods and their applicability are discussed. Experimental programming environments necessary for implementing trainable approaches are analyzed separately.},
	pages = {S79--S84},
	issue = {S1},
	journaltitle = {Doklady Mathematics},
	author = {Yakovlev, K. S. and Andreychuk, A. A. and Skrynnik, A. A. and Panov, Aleksandr I.},
	date = {2022},
	keywords = {appl, group1}
}


@article{Angulo2023,
  bibtex_show={true},
  abbr={RAL},
  dimensions={true},
  selected={true},
	title = {Policy Optimization to Learn Adaptive Motion Primitives in Path Planning With Dynamic Obstacles},
	volume = {8},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/10003648/},
	doi = {10.1109/LRA.2022.3233261},
	abstract = {This paper addresses the kinodynamic motion planning for non-holonomic robots in dynamic environments with both static and dynamic obstacles – a challenging problem that lacks a universal solution yet. One of the promising approaches to solve it is decomposing the problem into the smaller sub-problems and combining the local solutions into the global one. The crux of any planning method for non- holonomic robots is the generation of motion primitives that generates solutions to local planning sub-problems. In this work we introduce a novel learnable steering function (policy), which takes into account kinodynamic constraints of the robot and both static and dynamic obstacles. This policy is efficiently trained via the policy optimization. Empirically, we show that our steering function generalizes well to unseen problems. We then plug in the trained policy into the sampling-based and lattice-based planners, and evaluate the resultant {POLAMP} algorithm (Policy Optimization that Learns Adaptive Motion Primitives) in a range of challenging setups that involve a car-like robot operating in the obstacle-rich parking-lot en- vironments. We show that {POLAMP} is able to plan collision- free kinodynamic trajectories with success rates higher than 92\%, when 50 simultaneously moving obstacles populate the environment showing better performance than the state-of-the- art competitors.},
	pages = {824--831},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Angulo, Brian and Panov, Aleksandr and Yakovlev, Konstantin},
	date = {2023},
	eprinttype = {arxiv},
	eprint = {2212.14307},
	keywords = {robotics, group1}
}

@article{Skrynnik2022a,
  bibtex_show={true},
  abbr={PeerJ},
  dimensions={true},
  selected={false},
	title = {Pathfinding in stochastic environments: learning vs planning},
	volume = {8},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-1056},
	doi = {10.7717/peerj-cs.1056},
	abstract = {Among the main challenges associated with navigating a mobile robot in complex environments are partial observability and stochasticity. This work proposes a stochastic formulation of the pathfinding problem, assuming that obstacles of arbitrary shapes may appear and disappear at random moments of time. Moreover, we consider the case when the environment is only partially observable for an agent. We study and evaluate two orthogonal approaches to tackle the problem of reaching the goal under such conditions: planning and learning. Within planning, an agent constantly re-plans and updates the path based on the history of the observations using a search-based planner. Within learning, an agent asynchronously learns to optimize a policy function using recurrent neural networks (we propose an original efficient, scalable approach). We carry on an extensive empirical evaluation of both approaches that show that the learning-based approach scales better to the increasing number of the unpredictably appearing/disappearing obstacles. At the same time, the planning-based one is preferable when the environment is close-to-the-deterministic ( i.e. , external disturbances are rare). Code available at https://github.com/Tviskaron/pathfinding-in-stochastic-envs .},
	pages = {e1056},
	journaltitle = {{PeerJ} Computer Science},
	author = {Skrynnik, Alexey and Andreychuk, Anton and Yakovlev, Konstantin and Panov, Aleksandr},
	date = {2022},
	keywords = {appl, group1}
}

@article{Staroverov2022,
  bibtex_show={true},
  abbr={Access},
  dimensions={true},
  selected={true},
	title = {Hierarchical Landmark Policy Optimization for Visual Indoor Navigation},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9795006/},
	doi = {10.1109/ACCESS.2022.3182803},
	abstract = {In this paper, we study the problem of visual indoor navigation to an object that is defined by its semantic category. Recent works have shown significant achievements in the end-to-end reinforcement learning approach and modular systems. However, both approaches need a big step forward to be robust and practically applicable. To solve the problem of insufficient exploration of the scenes and make exploration more semantically meaningful, we extend standard task formulation and give the agent easily accessible landmarks in the form of the room locations and those types. The availability of landmarks allows the agent to build a hierarchical policy structure and achieve a success rate of 63\% on validation scenes in a photo- realistic Habitat simulator. In a hierarchy, a low level consists of separately trained {RL} skills and a high level deterministic policy, which decides which skill is needed at the moment. Also, in this paper, we show the possibility of transferring a trained policy to a real robot. After a bit of training on the reconstructed real scene, the robot shows up to 79\% {SPL} when solving the task of navigating to an arbitrary object.},
	pages = {70447--70455},
	journaltitle = {{IEEE} Access},
	author = {Staroverov, Aleksei and Panov, Aleksandr},
	date = {2022},
	keywords = {robotics, group1}
}

@article{Skrynnik2021b,
  bibtex_show={true},
  abbr={Access},
  dimensions={true},
  selected={true},
	title = {Hybrid Policy Learning for Multi-Agent Pathfinding},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9532001/},
	doi = {10.1109/ACCESS.2021.3111321},
	abstract = {In this work we study the behavior of groups of autonomous vehicles, which are the part of the Internet {ofVehicles} systems. One of the challenging modes of operation of such systems is the case when the observability of each vehicle is limited and the global/local communication is unstable, e.g. in the crowded parking lots. In such scenarios the vehicles have to rely on the local observations and exhibit cooperative behavior to ensure safe and efficient trips. This type of problems can be abstracted to the so-called multi- agent pathfinding when a group of agents, confined to a graph, have to find collision-free paths to their goals (ideally, minimizing an objective function e.g. travel time). Widely used algorithms for solving this problem rely on the assumption that a central controller exists for which the full state of the environment (i.e. the agents current positions, their targets, configuration of the static obstacles etc.) is known and they cannot be straightforwardly be adapted to the partially-observable setups. To this end, we suggest a novel approach which is based on the decomposition of the problem into the two sub-tasks: reaching the goal and avoiding the collisions. To accomplish each of this task we utilize reinforcement learning methods such as Deep Monte Carlo Tree Search, Q-mixing networks, and policy gradients methods to design the policies that map the agents’ observations to actions. Next, we introduce the policy-mixing mechanism to end up with a single hybrid policy that allows each agent to exhibit both types of behavior – the individual one (reaching the goal) and the cooperative one (avoiding the collisions with other agents). We conduct an extensive empirical evaluation that shows that the suggested hybrid-policy outperforms standalone stat-of-the-art reinforcement learning methods for this kind of problems by a notable margin.},
	pages = {126034--126047},
	journaltitle = {{IEEE} Access},
	author = {Skrynnik, Alexey and Yakovleva, Alexandra and Davydov, Vasilii and Yakovlev, Konstantin and Panov, Aleksandr I.},
	date = {2021},
	keywords = {appl, group1}
}

@incollection{Jamal2021,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Adaptive Maneuver Planning for Autonomous Vehicles Using Behavior Tree on Apollo Platform},
	volume = {13101},
	url = {https://link.springer.com/10.1007/978-3-030-91100-3_26},
	abstract = {In safety-critical systems such as autonomous driving sys- tems, behavior planning is a significant challenge. The presence of numerous dynamic obstacles makes the driving environment unpredictable. The planning algorithm should be safe, reactive, and adaptable to environmental changes. The paper presents an adaptive maneuver planning algorithm based on an evolving behavior tree created with genetic programming. In addition, we make a technical contribution to the Baidu Apollo autonomous driving platform, allowing the platform to test and develop overtaking maneuver planning algorithms.},
	pages = {327--340},
	booktitle = {Artificial Intelligence {XXXVIII}. {SGAI} 2021. Lecture Notes in Computer Science},
	author = {Jamal, Mais and Panov, Aleksandr},
	editor = {Bramer, Max and Ellis, Richard},
	date = {2021},
	doi = {10.1007/978-3-030-91100-3_26},
	keywords = {robotics, group1}
}


@incollection{Davydov2021,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Q-Mixing Network for Multi-agent Pathfinding in Partially Observable Grid Environments},
	volume = {12948},
	isbn = {978-3-030-86854-3},
	url = {https://arxiv.org/abs/2108.06148},
	abstract = {In this paper, we consider the problem of multi-agent navigation in partially observable grid environments. This problem is challenging for centralized planning approaches as they typically rely on full knowledge of the environment. To this end, we suggest utilizing the reinforcement learning approach when the agents first learn the policies that map observations to actions and then follow these policies to reach their goals. To tackle the challenge associated with learning cooperative behavior, i.e. in many cases agents need to yield to each other to accomplish a mission. We use a mixing Q-network that complements learning individual policies. In the experimental evaluation, we show that such approach leads to plausible results and scales well to a large number of agents.},
	pages = {169--179},
	booktitle = {Artificial Intelligence. {RCAI} 2021. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Davydov, Vasilii and Skrynnik, Alexey and Yakovlev, Konstantin and Panov, Aleksandr},
	editor = {Kovalev, Sergei M. and Kuznetsov, Sergei O. and Panov, Aleksandr I.},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2108.06148},
	doi = {10.1007/978-3-030-86855-0_12},
	keywords = {appl, group1}
}

@article{Staroverov2020b,
  bibtex_show={true},
  abbr={Access},
  dimensions={true},
  selected={true},
	title = {Real-Time Object Navigation with Deep Neural Networks and Hierarchical Reinforcement Learning},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9241850/},
	doi = {10.1109/ACCESS.2020.3034524},
	abstract = {In the last years, deep learning and reinforcement learning methods have significantly improved mobile robots in such fields as perception, navigation, and planning. But there are still gaps in applying these methods to real robots due to the low computational efficiency of recent neural network architectures and their poor adaptability to robotic experiments’ realities. In this paper, we consider an important task in mobile robotics - navigation to an object using an {RGB}-D camera.We develop a new neural network framework for robot control that is fast and resistant to possible noise in sensors and actuators. We propose an original integration of semantic segmentation, mapping, localization, and reinforcement learning methods to improve the effectiveness of exploring the environment, finding the desired object, and quickly navigating to it. We created a new {HISNav} dataset based on the Habitat virtual environment, which allowed us to use simulation experiments to pre-train the model and then upload it to a real robot. Our architecture is adapted to work in a real-time environment and fully implements modern trends in this area.},
	pages = {195608--195621},
	journaltitle = {{IEEE} Access},
	author = {Staroverov, Aleksey and Yudin, Dmitry A. and Belkin, Ilya and Adeshkin, Vasily and Solomentsev, Yaroslav K. and Panov, Aleksandr I.},
	date = {2020},
	keywords = {robotics, group1}
}

@article{Yudin2019,
  bibtex_show={true},
  abbr={OMNN},
  dimensions={true},
  selected={false},
	title = {Object Detection with Deep Neural Networks for Reinforcement Learning in the Task of Autonomous Vehicles Path Planning at the Intersection},
	volume = {28},
	issn = {1060-992X},
	url = {https://link.springer.com/article/10.3103%2FS1060992X19040118},
	doi = {10.3103/S1060992X19040118},
	abstract = {Among a number of problems in the behavior planning of an unmanned vehicle the central one is movement in difficult areas. In particular, such areas are intersections at which direct interac- tion with other road agents takes place. In our work, we offer a new approach to train of the intelligent agent that simulates the behavior of an unmanned vehicle, based on the integration of reinforcement learning and computer vision. Using full visual information about the road intersection obtained from aerial photographs, it is studied automatic detection the relative positions of all road agents with vari- ous architectures of deep neural networks ({YOLOv}3, Faster R-{CNN}, {RetinaNet}, Cascade R-{CNN}, Mask R-{CNN}, Cascade Mask R-{CNN}). The possibilities of estimation of the vehicle orientation angle based on a convolutional neural network are also investigated. Obtained additional features are used in the modern effective reinforcement learning methods of Soft Actor Critic and Rainbow, which allows to accelerate the convergence of its learning process. To demonstrate the operation of the devel- oped system, an intersection simulator was developed, at which a number of model experiments were carried out.},
	pages = {283--295},
	number = {4},
	journaltitle = {Optical Memory and Neural Networks},
	author = {Yudin, D A and Skrynnik, A and Krishtopik, A and Belkin, I and Panov, Aleksandr I.},
	date = {2019},
	keywords = {robotics, group1}
}

@incollection{Younes2019,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Toward Faster Reinforcement Learning for Robotics : Using Gaussian Processes},
	volume = {11866},
	url = {https://link.springer.com/chapter/10.1007%2F978-3-030-33274-7_11},
	abstract = {Standard robotic control works perfectly in case of ordinary conditions, but in the case of a change in the conditions (e.g. damaging of one of the motors), the robot won’t achieve its task anymore. We need an algorithm that provide the robot with the ability of adaption to unforeseen situations. Reinforcement learning provide a framework corresponds with that requirements, but it needs big data sets to learn robotic tasks, which is impractical. We discuss using Gaussian processes to improve the efficiency of the Reinforcement learning, where a Gaussian Process will learn a state transition model using data from the robot (interaction) phase, and after that use the learned {GP} model to simulate trajectories and optimize the robot’s controller in a (simulation) phase. {PILCO} algorithm considered as the most data efficient {RL} algorithm. It gives promising results in Cart-pole task, where a working controller was learned after seconds of (interaction) on the real robot, but the whole training time, considering the training in the (simulation) was longer. In this work, we will try to leverage the abilities of the computational graphs to produce a {ROS} friendly python implementation of {PILCO}, and discuss a case study of a real world robotic task.},
	pages = {160--174},
	booktitle = {{RAAI} Summer School 2019. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Younes, Ali and Panov, Aleksandr I},
	editor = {Osipov, Gennady S. and Panov, Aleksandr I. and Yakovlev, Konstantin S.},
	date = {2019},
	doi = {10.1007/978-3-030-33274-7_11},
	keywords = {robotics, group1}
}

@inproceedings{Kiselev2017a,
  bibtex_show={true},
  abbr={LNCS},
  dimensions={true},
  selected={false},
	title = {Synthesis of the Behavior Plan for Group of Robots with Sign Based World Model},
	volume = {10459},
	rights = {международная лицензия «Creative Commons Attribution-{NonCommercial}-{ShareAlike} 4.0» ({CC}-{BY}-{NC}-{SA})},
	isbn = {978-3-319-66470-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-66471-2_10},
	doi = {10.1007/978-3-319-66471-2_10},
	abstract = {The paper considers the task of the group’s collective plan intellectual agents. Robotic systems are considered as agents, possessing a manipulator and acting with objects in a determined external environment. The {MultiMAP} planning algorithm proposed in the article is hierarchical. It is iterative and based on the original sign representation of knowledge about objects and processes, agents knowledge about themselfs and about other members of the group. For distribution actions between agents in general plan signs “I” and “Other” (“They”) are used. In conclusion, the results of experiments in the model problem “Blocksworld” for a group of several agents are presented.},
	pages = {83--94},
	booktitle = {Interactive Collaborative Robotics. {ICR} 2017. Lecture Notes in Computer Science},
	publisher = {Springer},
	author = {Kiselev, Gleb A. and Panov, Aleksandr I.},
	editor = {Ronzhin, A. and Rigoll, G. and Meshcheryakov, R.},
	date = {2017},
	keywords = {robotics, group1}
}
@inproceedings{Zholus2023,
  bibtex_show={true},
  abbr={NeuroInfo},
  html={https://link.springer.com/10.1007/978-3-031-19032-2_3},
  pdf={tapig2022.pdf},
  dimensions={true},
  selected={false},
  title = {Addressing {Task} {Prioritization} in {Model}-based {Reinforcement} {Learning}},
  booktitle = {Advances in Neural Computation, Machine Learning, and Cognitive Research VI. NEUROINFORMATICS 2022. Studies in Computational Intelligence},
  author = {Zholus, Artem and Ivchenkov, Yaroslav and Panov, Aleksandr I},
  editor = {Kryzhanovsky, B. and Dunin-Barkowski, W. and Redko, V. and Tiumentsev, Y.},
  year = {2023},
  pages = {19--30},
  volume = {1064},
  isbn = {978-3-031-19031-5},
  url = {https://link.springer.com/10.1007/978-3-031-19032-2_3},
  doi = {10.1007/978-3-031-19032-2_3},
  abstract = {World models facilitate sample-efficient reinforcement learning (RL) and, by design, can benefit from the multitask information. However, it is not used by typical model-based RL (MBRL) agents. We propose a data-centric approach to this problem. We build a controllable optimization process for MBRL agents that selectively prioritizes the data used by the model-based agent to improve its performance. We show how this can favor implicit task generalization in a custom environment based on MetaWorld with a parametric task variability. Furthermore, by bootstrapping the agent’s data, our method can boost the performance on unstable environments from DeepMind Control Suite. This is done without any additional data and architectural changes outperforming state-of-the-art visual model-based RL algorithms. Additionally, we frame the approach within the scope of methods that have unintentionally followed the controllable optimization process paradigm, filling the gap of the data-centric task-bootstrapping methods.},
  keywords = {myconf, scopus, Reinforcement learning, frccsc, Model-based reinforcement learning, q4scopusprelim, airi, Generalization in RL, govgrant}
}

@inproceedings{Yudin2023,
  bibtex_show={true},
  abbr={ICONIP},
  html={https://link.springer.com/10.1007/978-3-031-19032-2_3},
  pdf={hpointloc2022.pdf},
  dimensions={true},
  selected={true},
	title = {HPointLoc: Point-based Indoor Place Recognition using Synthetic RGB-D Images},
	volume = {13625},
	url = {https://link.springer.com/chapter/10.1007/978-3-031-30111-7_40},
	doi = {10.1007/978-3-031-30111-7_40},
	abstract = {We present a novel dataset named as HPointLoc, specially designed for exploring capabilities of visual place recognition in indoor environment and loop detection in simultaneous localization and mapping. The loop detection sub-task is especially relevant when a robot with an on-board RGB-D camera can drive past the same place (``Point") at different angles. The dataset is based on the popular Habitat simulator, in which it is possible to generate photorealistic indoor scenes using both own sensor data and open datasets, such as Matterport3D. To study the main stages of solving the place recognition problem on the HPointLoc dataset, we proposed a new modular approach named as PNTR. It first performs an image retrieval with the Patch-NetVLAD method, then extracts keypoints and matches them using R2D2, LoFTR or SuperPoint with SuperGlue, and finally performs a camera pose optimization step with TEASER++. Such a solution to the place recognition problem has not been previously studied in existing publications. The PNTR approach has shown the best quality metrics on the HPointLoc dataset and has a high potential for real use in localization systems for unmanned vehicles. The proposed dataset and framework are publicly available: https://github.com/metra4ok/HPointLoc.},
	booktitle = {Neural Information Processing. Lecture Notes in Computer Science},
	author = {Yudin, Dmitry and Solomentsev, Yaroslav and Musaev, Ruslan and Staroverov, Aleksei and Panov, Aleksandr I},
	editor = {Tanveer, Mohammad and Agarwal, Sonali and Ozawa, Seiichi and Ekbal, Asif and Jatowt, Adam},
	year = {2023},
	keywords = {dataset, indoor localization, rgb-d image, synthetic, visual place recognition},
	pages = {471--484}
}

@article{Staroverov2023,
  bibtex_show={true},
  abbr={Robotics},
  html={https://www.mdpi.com/2218-6581/12/4/104},
  pdf={robotics2023.pdf},
  dimensions={true},
  selected={true},
	title = {Skill {Fusion} in {Hybrid} {Robotic} {Framework} for {Visual} {Object} {Goal} {Navigation}},
	volume = {12},
	issn = {2218-6581},
	url = {https://www.mdpi.com/2218-6581/12/4/104},
	doi = {10.3390/robotics12040104},
	abstract = {In recent years, Embodied AI has become one of the main topics in robotics. For the agent to operate in human-centric environments, it needs the ability to explore previously unseen areas and to navigate to objects that humans want the agent to interact with. This task, which can be formulated as ObjectGoal Navigation (ObjectNav), is the main focus of this work. To solve this challenging problem, we suggest a hybrid framework consisting of both not-learnable and learnable modules and a switcher between them—SkillFusion. The former are more accurate, while the latter are more robust to sensors’ noise. To mitigate the sim-to-real gap, which often arises with learnable methods, we suggest training them in such a way that they are less environment-dependent. As a result, our method showed top results in both the Habitat simulator and during the evaluations on a real robot. Video and code for our approach can be found on our website: https://github.com/AIRI-Institute/skill-fusion (accessed on 13 July 2023).},
	journal = {Robotics},
	author = {Staroverov, Aleksei and Muravyev, Kirill and Yakovlev, Konstantin and Panov, Aleksandr I},
	year = {2023}
}

@inproceedings{Ugadiarov2023,
  bibtex_show={true},
  abbr={IJCAI},
  pdf={ooqn2023.pdf},
  dimensions={true},
  selected={false},
	title = {Object-{Oriented} {Decomposition} of {World} {Model} in {Reinforcement} {Learning}},
	abstract = {Object-oriented models are expected to have better generalization abilities and operate on a more compact state representation. Recent studies have shown that using pre-trained object-centric representation learning models for state factorization in model-free algorithms improves the efficiency of policy learning. Approaches using object-factored world models to predict the environment dynamics have also shown their effectiveness in object-based grid-world environments. Following those works, we propose a novel object-oriented model-based value-based reinforcement learning algorithm Object Oriented Q-network (OOQN) employing an object-oriented decomposition of the world and state-value models. The results of the experiments demonstrate that the developed algorithm outperforms state-of-the-art model-free policy gradient algorithms and model-based value-based algorithm with a monolithic world model in tasks where individual dynamics of the objects is similar.},
	booktitle = {IJCAI Neuro-Symbolic Agents Workshop},
	author = {Ugadiarov, Leonid and Panov, Aleksandr I},
	year = {2023}
}

@inproceedings{Zemskova2023,
  bibtex_show={true},
  abbr={CVPR},
  pdf={segmatron2023.pdf},
  dimensions={true},
  selected={false},
	title = {SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment},
	abstract = {This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat Simulator. We showed that obtaining additional images using the agent’s actions in an indoor environment can improve the quality of semantic segmentation.},
	booktitle = {CVPR Workshop on Embodied AI},
	author = {Zemskova, Tatiana and Kichik, Margarita and Yudin, Dmitry and Panov, Aleksandr},
	year = {2023}
}
